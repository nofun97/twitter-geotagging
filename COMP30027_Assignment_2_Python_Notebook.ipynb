{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "def openTSV(filepath: str):\n",
    "    return pd.read_csv(filepath, sep='\\t')\n",
    "\n",
    "# function to extract certain keywords\n",
    "def extractKeywords(tweet: str):\n",
    "    stops = stopwords.words('english')\n",
    "    tw = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "    # remove all emoji\n",
    "    tweet = re.sub(r'\\\\[a-z0-9]{5}', '', tweet)\n",
    "\n",
    "    # remove all links\n",
    "    tweet = re.sub(r'(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?', '', tweet)\n",
    "    \n",
    "    # lower case all words\n",
    "    tweet.lower()\n",
    "    # extract and then remove hashtags\n",
    "    hashtags = re.findall(r\"#\\w+\", tweet)\n",
    "    tweet = re.sub(r'#\\w*', '', tweet)\n",
    "    \n",
    "    # extracting nouns and filtering stop words\n",
    "    tweet = ' '.join(tw.tokenize(tweet))\n",
    "    blob = TextBlob(tweet)\n",
    "    nouns = [word for word in blob.noun_phrases if word not in stops]\n",
    "    nouns = [Word(word) for word in nouns]\n",
    "    \n",
    "    # removing all unimportant punctuations\n",
    "    nouns = [word.strip(' /\\\\*.;,') for word in nouns if word.strip(' /\\\\*.;,') != '']\n",
    "    \n",
    "    # returning all nouns and hashtags\n",
    "    keywords = nouns + hashtags\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the first stage of preprocessing which just creates a tsv file that has ID, location, and keywords\n",
    "def preprocess(trainFilePath: str, storeFilePath: str=\"./data/preprocessed3.csv\"):\n",
    "    st = open(storeFilePath, 'w')\n",
    "    df = openTSV(trainFilePath)\n",
    "    \n",
    "    # writing headers\n",
    "    st.write('ID\\tLocation\\tkeywords\\n')\n",
    "    \n",
    "    # extracting keywords and writing them to the file\n",
    "    for i, row in df.iterrows():\n",
    "        keywords = extractKeywords(row[2])\n",
    "        if keywords == []:\n",
    "            continue\n",
    "        st.write(\"{}\\t{}\\t{}\\n\".format(i+1, row[1], ','.join(keywords)))\n",
    "\n",
    "    st.close()\n",
    "    return\n",
    "\n",
    "# removing unnecessary words, part of feature selection\n",
    "def deleteUnnecessaryWord(h):\n",
    "    x = defaultdict(lambda: defaultdict(int))         \n",
    "    \n",
    "    # iterate through each word\n",
    "    for k1 in h.keys():\n",
    "        sum = 0\n",
    "        # iterate through each user\n",
    "        for k2 in h[k1].keys():\n",
    "            sum += h[k1][k2]\n",
    "        \n",
    "        # only include if word is mentioned above certain number of times\n",
    "        if sum >= 10:\n",
    "            x[k1] = h[k1]\n",
    "    \n",
    "    # return the filtered keywords\n",
    "    return x\n",
    "\n",
    "# the second stage of preprocessing which turns each keyword into features for each instance\n",
    "def serializedPreprocessedData(tokenizedCSV: str, furtherPreprocess: str):\n",
    "    # calculate the frequency of each word of each user\n",
    "    fp = open(tokenizedCSV, 'r')\n",
    "    a = open(furtherPreprocess, 'w')\n",
    "    freq = defaultdict(lambda: defaultdict(int))\n",
    "    for i in fp:\n",
    "        x = i[:-1]\n",
    "        x = x.split('\\t')\n",
    "        # getting the keywords\n",
    "        keys = x[2].split(',')\n",
    "        \n",
    "        # adding the keyword for each user\n",
    "        for j in keys:\n",
    "            freq[j][x[0]] += 1\n",
    "    \n",
    "    # filtering the unnecessary keywords\n",
    "    freq = deleteUnnecessaryWord(freq)\n",
    "    \n",
    "    # storing data to a csv file\n",
    "    attr = sorted(freq.keys())\n",
    "    a.write(\"{},{},{}\\n\".format(\"ID\", ','.join(attr), \"Class\"))\n",
    "    fp.seek(0, 0)\n",
    "    fp.readline()\n",
    "    for i in fp:\n",
    "        # taking all data except class label\n",
    "        x = i[:-1].split('\\t')\n",
    "        \n",
    "        # keep appending feature data\n",
    "        a.write(\"{}\".format(x[0]))\n",
    "        for y in attr:\n",
    "            a.write(\",{}\".format(freq[y][x[0]]))\n",
    "        \n",
    "        # write class label\n",
    "        a.write(\",{}\\n\".format(x[1]))\n",
    "\n",
    "\n",
    "    fp.close()\n",
    "    a.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main data\n",
    "filepath = \"./data/train-raw.tsv\"\n",
    "\n",
    "# file to store data of first stage of preprocessing\n",
    "tokenizedCSV = \"./data/preprocessed.csv\"\n",
    "\n",
    "# file to store data of second stage of preprocessing\n",
    "furtherPreprocess = \"./data/further.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first stage preprocessing\n",
    "preprocess(filepath, tokenizedCSV)\n",
    "\n",
    "# second stage preprocessing\n",
    "serializedPreprocessedData(tokenizedCSV, furtherPreprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "# a function to change test data into dataframes for classifier use\n",
    "def fitTestData(fp: str, preprocessed: str):\n",
    "    data = open(fp, 'r')\n",
    "    preprocessed = open(preprocessed, 'r')\n",
    "    # getting the header\n",
    "    header = preprocessed.readline()\n",
    "    header = header.split(',')[1:-1]\n",
    "    \n",
    "    # user ids\n",
    "    ids = []\n",
    "    \n",
    "    # will contain all data\n",
    "    vals = {}\n",
    "    for i in header:\n",
    "        vals[i] = []\n",
    "        \n",
    "    # class labels\n",
    "    y = []\n",
    "    \n",
    "    # iterate through every row, avoiding header\n",
    "    data.readline()\n",
    "    for row in data:\n",
    "        x = row.split('\\t')\n",
    "        \n",
    "        # extracting keywords\n",
    "        keys = extractKeywords(x[2])\n",
    "        \n",
    "        # getting user id\n",
    "        ids.append(x[0])\n",
    "        \n",
    "        # getting class label\n",
    "        y.append(x[1])\n",
    "        \n",
    "        # calculating frequency of keyword for each instance\n",
    "        instance = {}\n",
    "        for i in keys:\n",
    "            if i not in instance:\n",
    "                instance[i] = 1\n",
    "            else:\n",
    "                instance[i] += 1\n",
    "        \n",
    "        # iterate through each keyword\n",
    "        for i in vals.keys():\n",
    "            # if an instance has a keyword, the frequency value is appended\n",
    "            if i in instance:\n",
    "                vals[i].append(instance[i])\n",
    "            # all the keywords not in instance are given zero value as it is not in instance\n",
    "            else:\n",
    "                vals[i].append(0)\n",
    "    # converting to dataframe and returning data\n",
    "    df = pd.DataFrame(vals)\n",
    "    return ids, df, y\n",
    "\n",
    "# function to process the processed training data and convert it to dataframe\n",
    "def fitTrainData(data: str):\n",
    "    data = pd.read_csv(data)\n",
    "    return data.iloc[:, 1:-1], data.iloc[:, -1]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: MultinomialNB, Score: 0.30338192732340014\n",
      "\n",
      "Classifier: DecisionTree, Score: 0.30338192732340014\n",
      "\n",
      "Classifier: BernoulliNB, Score: 0.30710687104727197\n",
      "\n",
      "Classifier: GaussianNB, Score: 0.296387608532533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# convert all data\n",
    "_, X_test, y_test = fitTestData(\"./data/dev-raw.tsv\", furtherPreprocess)\n",
    "X_train, y_train = fitTrainData(furtherPreprocess)\n",
    "\n",
    "# all the classifiers to use\n",
    "classifiers = {\"MultinomialNB\": MultinomialNB(), \"DecisionTree\": DecisionTreeClassifier(), \"BernoulliNB\": BernoulliNB(), \"GaussianNB\": GaussianNB()}\n",
    "\n",
    "# printing the classifiers accuracy\n",
    "for i in classifiers:\n",
    "    classifiers[i].fit(X_train, y_train)\n",
    "    score = classifiers[i].score(X_test, y_test)\n",
    "    print(\"Classifier: {}, Score: {}\\n\".format(i, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Results\n",
    " ### Without counting how many users use each word\n",
    " 1. Classifier: MultinomialNB, Score: 0.30338192732340014, Kaggle Score: 0.30205\n",
    " 2. Classifier: DecisionTree, Score: 0.3034623217922607, Kaggle Score: 0.28988\n",
    " 3. Classifier: BernoulliNB, Score: 0.30710687104727197, Kaggle Score: 0.29247\n",
    " 4. Classifier: GaussianNB, Score: 0.296387608532533\n",
    " \n",
    " \n",
    " ### With counting how many users use each word\n",
    " 1. Classifier: MultinomialNB, Score: 0.30316754207310537\n",
    " 2. Classifier: DecisionTree, Score: 0.30322113838567905\n",
    " 3. Classifier: BernoulliNB, Score: 0.3074820452352878\n",
    " 4. Classifier: GaussianNB, Score: 0.2960660306570908\n",
    " \n",
    " \n",
    " ### Using binary features (For Bernoulli Only)\n",
    " 1. Classifier: BernoulliNB, Score: 0.3074820452352878\n",
    " 2. Classifier: DecisionTree, Score: 0.3034623217922607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to test the test-raw.tsv\n",
    "ids, X_actualTest, _ = fitTestData(\"./data/test-raw.tsv\", furtherPreprocess)\n",
    "\n",
    "# creating prediction and store it appropriately\n",
    "predict = classifiers[\"MultinomialNB\"].predict(X_actualTest)\n",
    "pd.DataFrame({\"Id\": ids, \"Class\": list(predict)}).to_csv(\"./data/result.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
